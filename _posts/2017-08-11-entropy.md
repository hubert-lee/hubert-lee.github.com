---
layout: entry
title: 엔트로피(entropy)에 대해서...
author: 이동훈
author-email: voidtype@gmail.com
description: entorypy의 정의애 대해서 이해한 내용을 정리해보도록 합니다.
publish: true
categories : ['mechine learning']
tags : ['entropy','information theory']
---


# 들어가며...

딥러닝에서 하나의 학습데이터에 대해서 파라메터를 학습하고 난 후, 최종 레이블과 비교해서 얼마만큼의 오류가
있는지 확인하는 단계가 있습니다. 이때 이용되는 함수를 cost function이라고 부릅니다.

cost function은 정답과 가까울 수록 작은값을, 정답과 멀 수록 큰 값을 돌려주어서, 학습된 결과가 정답에 가까워질
수 있도록 안내하는 역할을 하는 것입니다.

cost function으로 자주 사용되는 예제는 MSE(mean squre error)와 cross entropy가 있습니다. 
제가 이해하고 있는 cross entropy는 

> 임의의 두 확률분포가 얼마만큼 차이가 있는지 정량적으로 나타낼 수 있는 척도

이정도로만 알고 있었는데, 이부분에 대해서 다시 이해를 해보고 싶었고, 우선은 엔트로피에 대한 정의부터 다시 
살펴볼 필요가 있었습니다.

그러던 차, 엔트로피에 대해서 [잘 정리된 글][0]을 발견하게 되었고요, 이 글을 읽고 이해한 내용을 잊지 않기 위해 다시
정리합니다. 

# 정보의 양을 나타내는 방법

예를 들어서, 철수가 영희에게 어떤 정보를 전달하기 위해서 한가지 사실을 말해주었는데, 이 말을 들은
영희는 철수가 말해준 사실에 정보량이 얼마만큼 들어있는지에 따라서 많이 놀라기도 하고, 그렇지 않을 수도 있을 것입니다.

철수가 전달한 내용은 불확실성을 포함하고 있고, 우리는 이것을 확률이라는 개념으로 표현을 합니다.
그래서 확률값이 1에 가까운 내용을 들으면, 거기에 정보량은 거의 없을 것이고, 확률값이 0에 가까운 내용을 들으면
정보량이 많다고 할수 있습니다.

철수가 이야기한 사실을 $$x$$라고 하고, 이 사실이 발생할 확률을 $$ p(x) $$로 나타낼 수 있습니다. $$ p(x) $$에 얼마만큼의 
정보량이 있는지는 다음 수식으로 계산할 수 있습니다.

$$ h(x) = -log_{2} p(x)$$

계산 예)
```
>>> p_x = 0.1
>>> h_x = -1 * math.log(p_x, 2.0)
>>> print h_x
    3.32192809489

>>> p_x = 0.9
>>> h_x = -1 * math.log(p_x, 2.0)
>>> print h_x
    0.152003093445
```

발생할 확률이 10%인 메세지에는 정보가 약 3.3만큼 담겨있고, 발생할 확률이 90%인 메세지에는 정보가 약 0.15만큼만 담겨있다고
볼 수 있을 것입니다. 즉, 흔하게 발생하는 사건일 수록 정보량이 작게 계산되는 것을 확인 할 수 있습니다.

로그 앞에 마이너스가 있는 이유는 1보다 작은 값에 로그를 취하면, 음수가 나오는데, 우리가 사용하는 단위는 양수로 만들어 주기
위해서 곱해져 있다고 보면 됩니다.

# 엔트로피의 정의

위에 정보량을 표현하는 방법을 알아보았는데요, 위 내용을 토대로 엔트로피에 대해서 정리해보도록 합니다.
[한국어 위키][1]에서는 엔트로피는 다음과 같이 정의하고 있습니다.

> 각 메시지에 포함된 정보의 기댓값(평균)이다

수식으로 나타내면 다음과 같습니다.

$$ H[x] = -\sum_{x} p(x)log_{2} p(x)  $$

제가 이해한 내용을 예제로 정리해보려고 합니다.
철수가 공정한 주사위를 하나 가지고 있습니다. 주사위를 한번 던져서 나온 값을 영희에게 전달한다고 했을 때, 영희에게 전달하는
정보량은 얼마나 되는지 알아보도록 하겠습니다.

철수가 가지는 메세지를 확률변수 $$ X $$ 로 나타내면 이 변수가 갖는 값은 $$ X = \{1,2,3,4,5,6\} $$ 총 6가지가 될 수 있습니다.
공정한 주사위 이므로, 각 사건이 발생할 확률은 다음과 같습니다. 

$$ p(X=x)={1\over 6} $$

모든 경우에 대한 정보량의 평균을 구하면 다음과 같습니다. 

$$ H[x] = -6 \times {1\over 6} \times log_{2}{1 \over 6} = 2.58496250072 $$  

```
>>> p_x = float(1) / float(6)
>>> h_x = -6 * p_x * math.log(p_x, 2.0)
>>> print h_x
2.58496250072
```

의미가 좀 이상하게 들릴 수는 있지만, 영희가 받은 메세지에 포함된 정보량은 약 2.58정도라고 할 수 있습니다.
여기서 우리가 사용한 로그가 밑이 2이므로, 이 정보량의 단위는 비트라고 표현하고 이 메세지를 표현하는데
최소한 2.58비트가 필요하다라는 것을 말해줍니다.

그런데, 주사위가 공정하지 못하다고 하면요, 예를 들어 1이 발생할 확률이 극단적으로 높은 주사위라고 가정해볼 수 있습니다.

그렇게 되면 확률변수가 갖는 값은 다음과 같은 분포를 갖을 것입니다.

$$ p(X=x)=\{ {5\over 6}, {1\over 30}, {1\over 30}, {1\over 30}, {1\over 30}, {1\over 30}\} $$

마찬가지로, 엔트로피를 계산해보면,

$$ 
H[x] = -{5\over 6} \times log_{2} {5 \over 6}    
       +[-5 \times {1\over 30} \times log_{2} {1 \over 30}] = 1.2486421827216125
$$

```
>>> p_x = [1/6, 1/30, 1/30, 1/30, 1/30, 1/30]
>>> h_x = sum([ -1 * x * math.log(x,2.0) for x in p_x])
>>> print(h_x)
1.2486421827216125
```

위 정보를 전달하는데 필요한 비트수는 1.2비트로 줄어든 것을 볼 수 있는데요, 
이 주사위는 1이 나올 확률이 극단적으로 높다라는 정보때문에 불확실성이 어느정도 해소가 되었고,
엔트로피가 더 낮아진 것을 확인할 수 있습니다. 그리고 이 주사위의 결과값을 표시하는 메세지는
더 적은 비트수로 이 메세지를 표현할 수 있다고 볼 수 있습니다.


# 마무리

맨날 봐도 헷갈리는 엔트로피에 대한 정리가 어느정도 된 것 같습니다.
엔트로피가 낮다 = 유의미한 정보가 있다. = 더 적은 비트수로 표현 가능하다.
엔트로피가 높다 = 정보가 없다 = 정보가 없으므로, 모든 경우를 표현해야하므로, 많은 비트수가 필요하다.


다음에는 cross entropy에 대해서 정리해볼 생각입니다.



[0]: http://norman3.github.io/prml/docs/chapter01/6
[1]: https://ko.wikipedia.org/wiki/%EC%A0%95%EB%B3%B4_%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC
